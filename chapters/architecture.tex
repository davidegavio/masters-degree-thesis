\providecommand{\main}{..}
\documentclass[\main/main.tex]{subfiles}

\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    numbers=left,
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    frame=lines,
    backgroundcolor=\color{background},
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}

\begin{document}
\chapter{Architecture}

\section{Service-Oriented Architecture}
SOA, or service-oriented architecture, defines a way to make software components reusable and interoperable via service interfaces. Services use common interface standards and an architectural pattern so they can be rapidly incorporated into new applications. The service interfaces provide loose coupling, meaning they can be called with little or no knowledge of how the service is implemented underneath, reducing the dependencies between applications. 
\cite{SOA_ibm}

\section{Microservices}
Microservices (or microservices architecture) are a cloud native architectural approach in which a single application is composed of many loosely coupled and independently deployable smaller components, or services. 
\cite{microservices_ibm}

\section{Project architecture}
The implemented architecture follows the idea of SOA or Microservices, with some differences. It's based on a series of services that read the input from data lake and writes the result on the data lake. The final result is a pipeline of services, executed one after the other.
\begin{figure}[h]
    \centering
    \includegraphics[scale=0.75]{images/architecture/architecture_service_example.png}
    \caption{Architecture example}
    \label{fig:architecture_example}
\end{figure}
Every service executed is a transaction, if there is an error in the execution there's no update in written data. Reading and writing to disk is the most efficient approach, because it avoids to break the memory managing big datasets. Using Python, time complexity scales linearly adding cluster nodes: for example, if a computation takes 30 minutes with 5 nodes, it takes 15 minutes with 10 nodes. 

\subsection{Categories of services}
In the implemented framework there are two main categories of services:
\begin{itemize}
    \item \textbf{Preprocessing services}: preprocessing services are services that prepare the dataset to be used. Those services cover the corrective operations that are applied on the original dataset: textual data is cleaned from special characters, documents are adapted in order to be used further on, documents are filtered given a condition, \dots  \\
    In this category there are two services: \emph{cleaning} and \emph{filtering} services.
    \item \textbf{Processing services}: processing services are services that constitutes the core of the framework: in those services, data is processed in order to produce usable embedding representation in order to perform a scoring algorithm to fulfill a semantic similarity search task. \\
    In this category there are three services: \emph{embedding extraction}, \emph{numerosity reduction} and \emph{scoring} services.
\end{itemize}

\subsection{DAG}
Once defined the services that need to be executed, these make up a so called Direct Acyclical Graph of execution: it consists in a pipeline where the result of a service execution is the input for the subsequent service. Ideally, the correct form of the DAG is the following:
\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{images/architecture/dag_theory.png}
    \caption{DAG example}
    \label{fig:dag_example}
\end{figure}
As visible in Figure \ref{fig:dag_example}, the two phases are subsequent and linked. If anything breaks the pipeline, the dataset will be updated to the last written result, allowing to restart from that checkpoint and not recomputing from the beginning, saving up precious machine-time.
\subsection{Orchestrator}
An orchestrator is a module of a framework which has the task of managing the execution of other modules in order to guarantee the right pipeline execution. The orchestrator reads a series of messages that parametrize different services, than executes the associated services passing read messages as service input. Every message is a correctly formatted JSON object which contains all values that are necessary. An example of parameter message is visible below:
\\
\begin{center}
    \begin{lstlisting}[language=json, caption="Parameter message example", captionpos=b]
        {
         "service_name": "test_service",
         "parameter_0": "value",
         "parameter_1": "value",
         ...,
         ...,
         "parameter_n": "value,
         "hyperparameters": ["hyperparameter_0", 
                             "hyperparameter_1", 
                             ..., 
                             "hyperparameter_m"]
        }
    \end{lstlisting}
\end{center}

Each one of the above parameters and hyperparameters are passed to the associated service in order to instantiate different variables that will drive the service execution.
The pseudocode for the orchestrator is visible below:
\begin{center}
    \begin{algorithm}[H]
     \KwData{List of JSON messages}
     \KwResult{Execution of services}
     initialization\;
     \While{not at end of message list}{
      read message\;
      call right service\;
      pass current message as input\;
     }
     \caption{Orchestrator pseudocode}
    \end{algorithm}
\end{center}

\end{document}