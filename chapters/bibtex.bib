@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{song2020mpnet,
  title={Mpnet: Masked and permuted pre-training for language understanding},
  author={Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2004.09297},
  year={2020}
}

@article{lample2019cross,
  title={Cross-lingual language model pretraining},
  author={Lample, Guillaume and Conneau, Alexis},
  journal={arXiv preprint arXiv:1901.07291},
  year={2019}
}

@article{zaharia2010spark,
  title={Spark: Cluster computing with working sets.},
  author={Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J and Shenker, Scott and Stoica, Ion and others},
  journal={HotCloud},
  volume={10},
  number={10-10},
  pages={95},
  year={2010}
}

@article{dean2004mapreduce,
  title={MapReduce: Simplified data processing on large clusters},
  author={Dean, Jeffrey and Ghemawat, Sanjay},
  year={2004}
}

@book{leskovec_rajaraman_ullman_2020, 
    place={Cambridge}, 
    edition={3}, 
    title={Mining of Massive Datasets}, 
    DOI={10.1017/9781108684163}, 
    publisher={Cambridge University Press}, 
    author={Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David}, year={2020}}
    
@misc{artea,
      title = {artea.com},
      howpublished = {\url{https://artea.com/}}
}

@incollection{cosine_HAN201239,
            title = {2 - Getting to Know Your Data},
            editor = {Jiawei Han and Micheline Kamber and Jian Pei},
            booktitle = {Data Mining (Third Edition)},
            publisher = {Morgan Kaufmann},
            edition = {Third Edition},
            address = {Boston},
            pages = {39-82},
            year = {2012},
            series = {The Morgan Kaufmann Series in Data Management Systems},
            isbn = {978-0-12-381479-1},
            doi = {https://doi.org/10.1016/B978-0-12-381479-1.00002-2},
            url = {https://www.sciencedirect.com/science/article/pii/B9780123814791000022},
            author = {Jiawei Han and Micheline Kamber and Jian Pei}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@article{Gandomi2015BeyondTH,
  title={Beyond the hype: Big data concepts, methods, and analytics},
  author={A. Gandomi and Murtaza Haider},
  journal={Int. J. Inf. Manag.},
  year={2015},
  volume={35},
  pages={137-144}
}

@misc{bigdatagartner,
      title = {Big Data},
      howpublished = {\url{https://www.gartner.com/en/information-technology/glossary/big-data}}
}

@misc{SOA_ibm,
      title = {SOA (Service-Oriented Architecture)},
      howpublished = {\url{https://www.ibm.com/cloud/learn/soa}}
}

@misc{microservices_ibm,
      title = {Microservices},
      howpublished = {\url{https://www.ibm.com/cloud/learn/microservices}}
}

@misc{html_parser,
      title = {html.parser â€” Simple HTML and XHTML parser},
      howpublished = {\url{https://docs.python.org/3/library/html.parser.html}}
}

