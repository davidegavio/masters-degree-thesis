@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{song2020mpnet,
  title={Mpnet: Masked and permuted pre-training for language understanding},
  author={Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2004.09297},
  year={2020}
}

@article{lample2019cross,
  title={Cross-lingual language model pretraining},
  author={Lample, Guillaume and Conneau, Alexis},
  journal={arXiv preprint arXiv:1901.07291},
  year={2019}
}

@article{zaharia2010spark,
  title={Spark: Cluster computing with working sets.},
  author={Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J and Shenker, Scott and Stoica, Ion and others},
  journal={HotCloud},
  volume={10},
  number={10-10},
  pages={95},
  year={2010}
}

@article{dean2004mapreduce,
  title={MapReduce: Simplified data processing on large clusters},
  author={Dean, Jeffrey and Ghemawat, Sanjay},
  year={2004}
}

@book{leskovec_rajaraman_ullman_2020, 
    place={Cambridge}, 
    edition={3}, 
    title={Mining of Massive Datasets}, 
    DOI={10.1017/9781108684163}, 
    publisher={Cambridge University Press}, 
    author={Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David}, year={2020}}
