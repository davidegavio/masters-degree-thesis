\providecommand{\main}{..}
\documentclass[\main/main.tex]{subfiles}
\begin{document}
\chapter{Conclusions}
\section{Project conclusions}
Wrapping up, semantic search task relies heavily on the concept of semantic similarity. When thinking outside computer science, the concept of semantic similarity is about finding a common meaning between two sentences, documents, \dots, while possibly having different words and expression within. This research aimed to explore the opportunities to exploit a semantic technology like transformers in order to enhance semantic search in a scope, like the project one, where compared textual data are different from each other.
The Proof of Concept (PoC) presented in this document consists of a ``Semantic Search Engine'' able to semantically rank documents given a query at search time based entirely on Transformer technology, able to capture the context and the relations inside given documents. \\
It has been demonstrated throughout the project development that an approach like the one described in this document could be helpful in context where there is a big quantity of documents and a human has to manually search within this corpus. Using this framework is possible to retrieve the most pertinent result(s) for a given query. \\
An important aspect that emerges from the document is that the model choice is absolutely crucial: the more specialized the model is for the task, the better the results obtainable. As visible in Chapter 5, using generic models such as \emph{distilbert-base-multilingual-cased} or \emph{albert-base-v2} led to inconsistent results, while using a model like \emph{stsb-mpnet-base-v2} which is designed to work well in similarity tasks return better results. \\
In conclusion, during the course of this thesis, satisfactory results were achieved and has been developed a software solution able to take care of all aspects pertaining to NLP, from data extraction to result evaluation. This piece of software provides, even potentially to not expert, a solution to the problem of semantic similarity in big data and is composed of:
\begin{enumerate}
    \item a preprocessing engine able to clean and reorganize the content of a document to be more digestible for the processing engine;
    \item a processing engine able to take well formed documents and rank them in order to provide the best result for a given input query;
    \item an evaluation engine able to estimate quantitatively how good are the choices that the user made in terms of hyperparameters.
\end{enumerate}
The PoC leverages the most cutting edge technologies like Transformers, Spark and HDFS in order to deliver a solution able to be up-to-date with the evolution of the subject: the framework is able to read different kind of documents even if not well formed and it is able to scale thanks to the distributed computing technology adopted not needing any code modification. 


\section{Future work}
As said in Chapter 4, due to time constraints, it hasn't been possible to test the system on a bigger portion of the dataset so a possible further improvement could be given by the application of the framework on a different and bigger portion of the dataset. \\
As said in Chapter 1, one of the main challenges has been dealing with long documents: an improvement to this work could be testing a different kind of document vectorization, perhaps adding a previous information retrieval layer able to extract relevant text portions and then vectorize only these parts, reducing computational and memory load. Speaking about the algorithmic part of the project, every further possible enhancement could constitute a future work. Supervised approaches and different similarity computation methods could bring benefit to the final result.\\
Another future work, should be testing the framework on a dataset coming from a completely different domain. As previously stated, the system is able to work with every kind of textual document from a technical point of view, but speaking about performances it is not as easy as it seems: the best model for the current dataset could be the worst for another dataset, the cleaning approach applied could be harmful when applied on another dataset and so on.\\
Wrapping the framework inside an application architecture to allow the runtime computation of needed similarities will be another important future work. \\
Lastly, an interesting contribution could be specializing a chosen model for the current dataset in order to theoretically obtain even better result than the starting model.
\newpage
\newpage
\end{document}