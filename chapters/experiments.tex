\providecommand{\main}{..}
\documentclass[\main/main.tex]{subfiles}
\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    numbers=left,
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    frame=lines,
    backgroundcolor=\color{background},
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}
\begin{document}
\chapter{Experiments and results}
\section{Experimental setup}
\subsection{Dataset}
Due to computation time constraints, it has not been possible to perform experiments on the whole dataset. Thus, the dataset has been reduced in order to make possible the execution of different experiments in a reasonable time. As mentioned before in \emph{Methods} chapter, a resulting dataset of 100 records has been created. Each record is a key-value pair object composed in the following way:
\begin{center}
    \begin{lstlisting}[language=json, caption="Experimental dataset object", captionpos=b]
        {
         "query_document_key": "query_key",
         "query_field_0": "value",
         ...: ...,
         "query_field_n": "value",
         "search_document_key": "search_key"
         "search_field_0": "value",
         ...: ...,
         "search_field_n": "value",
         "relation": "true"
        }
    \end{lstlisting}
\end{center}
All the informative content is maintained. At this point, the memory footprint of the dataset is at his peak, because it contains all the previous fields both of the original objects, query and search documents.\\
As previously mentioned, the dataset is composed by one hundred query-search documents coupled: these couples have been decided in a qualitative way, choosing the right documents for each query. For each query there are five documents that are strongly correlated to it and five documents that are not correlated at all. This organization is useful both form the point of view of evaluation (as previously mentioned in \emph{Methods} chapter), but it is also useful from a computational point of view, given that the dataset is rather small.
An idea of the resulting dataset is visible below, for sake of brevity some information have been omitted:
\begin{center}
    \begin{tabular}{||c | c c||} 
     \hline
     Query key & Document key & Relation\\ [0.5ex] 
     \hline\hline
     query\_key\_0 & document\_key & true \\ 
     \hline
     \dots & \dots & \dots\\
     \hline
     query\_key\_0 & document\_key & false\\
     \hline
     \dots & \dots & \dots\\
     \hline
     query\_key\_9 & document\_key & true \\ 
     \hline
     \dots & \dots & \dots\\
     \hline
     query\_key\_9 & document\_key & false\\
     \hline
    \end{tabular}
\end{center}
As visible, for each query identified by \emph{query\_key\_n}, there are multiple documents associated with their correlation attribute.
\subsection{Transformer models}
As said in \emph{Methods} chapter, the model choice is the biggest discriminant for the final result. In \emph{Technologies} chapter has been presented the original transformer architecture based on Vaswani's paper \cite{vaswani2017attention}. Such technology has been further enhanced in order to obtain way more usable and representative embeddings as described in \emph{Methods}. 
\subsection{Libraries and modules}
\subsubsection{NumPy}
NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more\cite{harris2020array}.
\paragraph{The \emph{ndarray}} Is the core of the NumPy package, which encapsulates n-dimensional arrays of homogeneous data types. There are several important differences between NumPy arrays and the standard Python sequences:
\begin{itemize}
    \item \textbf{Array size}: have a fixed size at creation, unlike Python lists (which can grow dynamically). Changing the size of an ndarray will create a new array and delete the original
    \item \textbf{Element type}: elements in a NumPy array are all required to be of the same data type, and thus will be the same size in memory
    \item \textbf{Mathematical operations}: NumPy arrays facilitate advanced mathematical and other types of operations on large numbers of data. Typically, such operations are executed more efficiently and with less code than is possible using Python’s built-in sequences
    \item \textbf{Advanced libraries}: the growing plethora of scientific and mathematical Python-based packages are using NumPy arrays; though these typically support Python-sequence input, they convert such input to NumPy arrays prior to processing, and they often output NumPy arrays. In other words, in order to efficiently use much (perhaps even most) of today’s scientific/mathematical Python-based software, just knowing how to use Python’s built-in sequence types is insufficient - one also needs to know how to use NumPy arrays
\end{itemize}
\subsubsection{SciPy}
SciPy is a collection of mathematical algorithms and convenience functions built on the NumPy extension of Python. It adds significant power to the interactive Python session by providing the user with high-level commands and classes for manipulating and visualizing data. With SciPy, an interactive Python session becomes a data-processing and system-prototyping environment rivaling systems, such as MATLAB, IDL, Octave, R-Lab, and SciLab.
The additional benefit of basing SciPy on Python is that this also makes a powerful programming language available for use in developing sophisticated programs and specialized applications. Scientific applications using SciPy benefit from the development of additional modules in numerous niches of the software landscape by developers across the world. Everything from parallel programming to web and data-base subroutines and classes have been made available to the Python programmer. All of this power is available in addition to the mathematical libraries in SciPy\cite{2020SciPyNMeth}.
\subsubsection{Transformers}
Huggingface Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. In the project has been used the tokenizer associated to a given model in order to count how many tokens a sentence produces\cite{Wolf_Transformers_StateoftheArt_Natural_2020}.
\subsubsection{scikit-learn}
Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities\cite{scikit-learn}.
\subsubsection{NetworkX}
NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. With NetworkX one can load and store networks in standard and nonstandard data formats, generate many types of random and classic networks, analyze network structure, build network models, design new network algorithms, draw networks, and much more\cite{SciPyProceedings_11}.
\subsubsection{html, re}
\begin{itemize}
    \item \textbf{html}: this module defines a class HTMLParser which serves as the basis for parsing text files formatted in HTML (HyperText Mark-up Language) and XHTML. It has been used to clean up all the HTML tags and special characters present inside the body of the textual documents \cite{html_parser}
    \item \textbf{re}: this module provides regular expression matching operations. Regular expressions are characters patterns used to match different portion of a string. It is possible to manage these sub-strings, replacing, deleting, modifying the characters within. This module is heavily used for cleaning and splitting operations \cite{re}
\end{itemize}
\subsubsection{SentenceTransformers}
SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. This library abstracts some of the biggest problems of transformers, like sentence length, embedding averaging, even similarity is provided inside the package. In this project, has been only used the embedding extraction part that takes car of all the concerning aspects \cite{sentence-transformers}.
\subsubsection{Plotly}
Plotly's \cite{plotly} Python graphing library makes interactive, publication-quality graphs. Built on top of the Plotly JavaScript library (plotly.js), plotly enables Python users to create beautiful interactive web-based visualizations that can be displayed in Jupyter notebooks, saved to standalone HTML files, or served as part of pure Python-built web applications using Dash. Plotly became useful for the realization of some more interactive graphs and charts, like when graphs have been plotted for debug purpose.
\subsection{Cluster settings}
In order to perform all the experiments, these are the settings:
\begin{itemize}
    \item \textbf{Environment}: Databricks
    \item \textbf{Cluster dimension}: 13 nodes cluster
    \item \textbf{Programming language}: Python=3.7.10
    \item \textbf{Spark version}: PySpark=3.1.2
    \item \textbf{Libraries}: 
        \begin{itemize}
            \item numpy=1.18.1
            \item scikit-learn=0.22.1
            \item transformers=4.10.2
            \item sentence-transformers=2.0.0
            \item html=3.4
            \item re=2.2.1
            \item scipy=1.7.1
            \item networkx=2.6.3
            \item plotly=5.3.1
        \end{itemize}
\end{itemize}
This setting allowed to execute an entire experiment, so from dataset cleaning to evaluation, in a time between one and three hours, depending on the transformer model.

\section{Results}
As previously mentioned, the hyperparameters combination led to a series of different experiments. In the 52 performed experiments, there are great differences among hyperparameters settings.\\
For readability reasons, each model name has been converted to a shorter and more readable label:
\begin{table}[H]
    \centering
    \begin{tabular}{| c | c | c |}
        \hline
        Label & Model & Table entry\\ 
        \hline
        STSBMpnetBaseV2 & stsb-mpnet-base-v2 & Mpnet     \\
        ParaphraseAlbertBaseV2 & paraphrase-albert-base-v2 & PAlbert     \\
        ParaphraseXlmRMultilingualV1 & paraphrase-xlm-r-multilingual-v1 & Xlm    \\
        AlbertBaseV2 & albert-base-v2 & Albert     \\
        DistilbertBaseMultilingualCased & distilbert-base-multilingual-cased & Distilbert     \\ 
        \hline
    \end{tabular}
    \caption{Model name - Model label - Table entry correspondence}
    \label{tab:model_name_model_label}
\end{table}

\subsection{Result discussion}
In this subsection there will be a discussion about some important aspects of results that have emerged by our experiments. \\
The first important aspect that emerges from performed experiments is that models trained using two specific datasets achieve better results overall than other models. As visible from the table, using models trained using a set of sentence pairs with a similarity score, like \emph{stsb-mpnet-base-v2}, and models trained using paraphrase datasets, like \emph{paraphrase-albert-base-v2}, show better result overall than other generic models and are visible in Table \ref{tab:experimental_results_best}. Those models show a very good result for \emph{Precision@100}, meaning that the framework is able to assign right documents to right queries, but it's interesting to notice also that they can separate quite well right documents form wrong ones, as visible looking at \emph{Difference mean} and \emph{Difference median} metrics.\\
Secondly, the implemented numerosity reduction shows a result conservation useful if memory usage is a concern: reducing space occupation and maintaining similar result is considerable as an achievement. Looking at the first line of the table below, which is the best result achieved, is visible how reducing of a 10\% the number of sentences in the corpus doesn't influence the final result. When dealing with big data this could become useful to reduce space and memory occupation. Obviously, this approach works well on a dataset built like the one used during the project, it needs further tests to see how it behaves on differently built datasets.\\
Lastly, not furtherly specialized models like \emph{albert-base-v2} and \emph{distilbert-base-multilingual-cased} show worse results than others. In the project scope, is demonstrated that using those models doesn't reach good enough performances, emphasizing even more the concept that the model choice is crucial for a system like the one presented in this document and that the choice operation needs a big attention in order to make the best choice.

Table \ref{tab:experimental_results_complete} gathers the whole set of experimental result:\\
\paragraph{Table legend}
\begin{itemize}
    \item \textbf{Model name} (Model): a label given starting from the original model name
    \item \textbf{Precision@100} (P@100): as previously mentioned in \emph{Methods} chapter, this metric measures the percentage of right positioned documents in relation to the input query;
    \item \textbf{Difference mean} (Diff mean): as previously mentioned in \emph{Methods} chapter, this is the difference between the mean of correlated documents and not-correlated documents. The higher the result, the higher the framework ability to recognize right documents from wrong;
    \item \textbf{Difference median} (Diff median): as previously mentioned in \emph{Methods} chapter, this is the difference between the median of correlated documents and not-correlated documents. The higher the result, the higher the framework ability to recognize right documents from wrong;
    \item \textbf{Multilingual} (Multilang): if the model is multilingual or not;
    \item \textbf{Reduction ratio} (Red ratio \%): the percentage of vectors removed;
    \item \textbf{Voting technique} (Voting tech): the operation that aggregates the best results from voting, mean or median;
    \item \textbf{Reduction percentage} (Red \%): every model produces a different range of similarity values when it come to reduce the numerosity. In order to compute and to compare the results, the reduction portion is calculated as the percentage of the range of values.
\end{itemize}

\newpage

\begin{table}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{||c|c|c|c|c|c|c|c||}
\hline
Model & P@100 & Diff mean & Diff median & Multilang & Red ratio \% & Voting tech & Red \% \\
\hline\hline
\rowcolor{gray} Mpnet             & 0,88 & 0,13  & 0,12  & false & 9,88  & median & 0,4 \\
Mpnet                 & 0,88 & 0,13  & 0,13  & false & 0     & median & 0,1 \\
Mpnet                 & 0,88 & 0,13  & 0,13  & false & 0     & median & 0,2 \\
Mpnet                 & 0,88 & 0,13  & 0,13  & false & 0     & median & 0,3 \\
Mpnet                 & 0,88 & 0,13  & 0,13  & false & 0     & median & 0   \\
PAlbert          & 0,86 & 0,1   & 0,11  & false & 0     & median & 0,2 \\
\rowcolor{gray} PAlbert          & 0,86 & 0,1   & 0,11  & false & 6,52  & median & 0,4 \\
PAlbert          & 0,86 & 0,1   & 0,11  & false & 0     & median & 0   \\
Xlm    & 0,86 & 0,12  & 0,1   & true  & 0     & median & 0   \\
PAlbert          & 0,86 & 0,1   & 0,11  & false & 0,74  & median & 0,3 \\
PAlbert          & 0,86 & 0,1   & 0,11  & false & 0     & median & 0,1 \\
\rowcolor{gray} Mpnet                 & 0,86 & 0,12  & 0,12  & false & 11,49 & median & 0,5 \\
\rowcolor{gray} PAlbert          & 0,84 & 0,1   & 0,1   & false & 2,63  & median & 0,5 \\
Xlm    & 0,84 & 0,11  & 0,09  & true  & 3,64  & median & 0,5 \\
Mpnet                 & 0,84 & 0,12  & 0,12  & false & 0     & mean   & 0,2 \\
Mpnet                 & 0,84 & 0,12  & 0,12  & false & 0     & mean   & 0,1 \\
\rowcolor{gray} PAlbert          & 0,84 & 0,09  & 0,1   & false & 10,87 & mean   & 0,5 \\
Mpnet                 & 0,84 & 0,12  & 0,12  & false & 8,62  & mean   & 0,4 \\
Mpnet                 & 0,84 & 0,12  & 0,12  & false & 6,4   & mean   & 0,3 \\
\rowcolor{gray} Mpnet                 & 0,84 & 0,12  & 0,12  & false & 15,72 & mean   & 0,5 \\
Mpnet                 & 0,84 & 0,12  & 0,12  & false & 0     & mean   & 0   \\
PAlbert          & 0,82 & 0,1   & 0,11  & false & 0     & mean   & 0,2 \\
PAlbert          & 0,82 & 0,1   & 0,11  & false & 0     & mean   & 0,1 \\
PAlbert          & 0,82 & 0,1   & 0,11  & false & 5,56  & mean   & 0,3 \\
PAlbert          & 0,82 & 0,1   & 0,11  & false & 0     & mean   & 0   \\
PAlbert          & 0,82 & 0,1   & 0,11  & false & 2,67  & mean   & 0,4 \\
Xlm    & 0,8  & 0,1   & 0,1   & true  & 1,69  & mean   & 0,5 \\
Xlm    & 0,8  & 0,11  & 0,11  & true  & 0     & mean   & 0   \\
Albert                    & 0,74 & 0,02  & 0,02  & false & 0     & median & 0   \\
Distilbert & 0,74 & 0,04  & 0,05  & true  & 0     & median & 0   \\
\hline
\end{tabular}
}
\caption{Experimental results}
\label{tab:experimental_results_complete}
\end{table}

\begin{table}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{||c|c|c|c|c|c|c|c||}
\hline
Model & P@100 & Diff mean & Diff median & Multilang & Red ratio \% & Voting tech & Red \% \\
\hline\hline
Albert                    & 0,72 & 0,02  & 0,02  & false & 0     & mean   & 0   \\
Distilbert & 0,72 & 0,04  & 0,04  & true  & 0     & mean   & 0   \\
Distilbert & 0,68 & 0,02  & 0,02  & true  & 79,73 & median & 0,1 \\
Distilbert & 0,64 & 0,01  & 0,03  & true  & 93,75 & mean   & 0,4 \\
Distilbert & 0,64 & 0,01  & 0,03  & true  & 96,52 & median & 0,4 \\
Distilbert & 0,62 & 0,01  & 0,02  & true  & 67,21 & mean   & 0,1 \\
Distilbert & 0,58 & 0     & 0,03  & true  & 97,47 & median & 0,5 \\
Distilbert & 0,58 & 0     & 0,03  & true  & 97,22 & mean   & 0,5 \\
Albert                    & 0,54 & 0 & 0,02  & false & 96,67 & mean   & 0,1 \\
Albert                    & 0,54 & 0 & 0,02  & false & 96,15 & median & 0,1 \\
Distilbert & 0,54 & 0,01  & 0     & true  & 84,48 & mean   & 0,2 \\
Distilbert & 0,5  & 0     & 0     & true  & 87,5  & median & 0,3 \\
Distilbert & 0,5  & 0,01  & 0     & true  & 71,88 & median & 0,2 \\
Distilbert & 0,5  & 0     & 0     & true  & 94,74 & mean   & 0,3 \\
Albert                    & 0,48 & 0     & 0     & false & 96,08 & mean   & 0,5 \\
Albert                    & 0,48 & 0     & 0 & false & 94,44 & mean   & 0,3 \\
Albert                    & 0,48 & 0     & 0     & false & 95    & median & 0,5 \\
Albert                    & 0,48 & 0     & 0 & false & 94,44 & median & 0,3 \\
Albert                    & 0,48 & 0 & 0 & false & 95,56 & mean   & 0,2 \\
Albert                    & 0,48 & 0     & 0     & false & 96,08 & mean   & 0,4 \\
Albert                    & 0,48 & 0 & 0 & false & 97,37 & median & 0,2 \\
Albert                    & 0,48 & 0     & 0     & false & 96,08 & median & 0,4 \\
\hline
\end{tabular}
}
\caption{Continued from previous table}
\end{table}

\begin{table}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{||c|c|c|c|c|c|c|c||}
\hline
Model & P@100 & Diff mean & Diff median & Multilang & Red ratio \% & Voting tech & Red \% \\
\hline\hline
Mpnet             & 0,88 & 0,13  & 0,12  & false & 9,88  & median & 0,4 \\
PAlbert          & 0,86 & 0,1   & 0,11  & false & 6,52  & median & 0,4 \\
Mpnet                 & 0,86 & 0,12  & 0,12  & false & 11,49 & median & 0,5 \\
PAlbert          & 0,84 & 0,1   & 0,1   & false & 2,63  & median & 0,5 \\
PAlbert          & 0,84 & 0,09  & 0,1   & false & 10,87 & mean   & 0,5 \\
Mpnet                 & 0,84 & 0,12  & 0,12  & false & 15,72 & mean   & 0,5 \\
\hline
\end{tabular}
}
\caption{Best results}
\label{tab:experimental_results_best}
\end{table}

\end{document}